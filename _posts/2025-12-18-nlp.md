# 自然语言处理入门手册——以社科为例

## 写在前面

这是一本我在2022年写的手册，当时我还是一个刚入行一年多的算法工程师。写下这本书只是为了共享知识，以及纪念我在公司学到的一切。

# 第一章：自然语言与计算机

自然语言处理是一门融合了计算机科学、人工智能以及语言学的交叉学科。自然语言处理旨在通过计算机的技术和人工智能的算法，让计算机处理人类的语言并理解人类的语言。由于自然语言本身的复杂性，自然语言处理一直是一个非常艰难的问题。

语言是一个非常灵活的东西，同一件事情可以有很多种表达的方式，而同样的几个字，放在不同的语境下，可能会有不同的解释。所以，从循序渐进的角度而言，自然语言的处理经历了字词层面上的处理，到后来引入深度学习后语义层面上的处理，已经能够一点点地理解语言的含义了。

作为自然语言处理的对象，我们首先需要理清楚自然语言都有什么特征。

---

## 1.1 自然语言的特征

### 1.1.1 词汇量大
自然语言作为我们日常沟通、描述世界的语言，至少自然界的万千事物都需要我们用语言去描述。所以我们能够使用的词汇量是无穷无尽的。除了厚厚的词典以外，我们每天也在创造各种类型的新词。所以，自然语言的词汇量是非常大的。

在实际做自然语言处理的时候，面临的一个很大的问题就是 OOV，即不在词典里的词。我们拿到一份很大的语料时，里面蕴藏着很多未知且重要的词汇。

在社科语料里，OOV 往往不只是“新词”，还可能是缩写、黑话、谐音、代称、以及各种专有名词（人名、组织名、事件名）。这些词如果被切碎了，后面做统计就会很难受，所以很多时候你第一步要做的不是“高级模型”，而是先把关键词保住。

---

### 1.1.2 非结构化
结构化与非结构化是相对的。结构化的数据是整理好的数据，例如我们平时制作的 Excel 表格，里面规规整整的数据就是结构化的。比如一家公司的结构化信息如下：

- 公司名：腾讯  
- CEO：马化腾  

但是我们用自然语言去表述的时候，就没有这种层次化的模板，我们的自然语言可能是“腾讯的 CEO 是马化腾”，或者“小马哥是腾讯的老大”。这种语句如果要计算机转化成结构化的信息，除了分词，还需要做命名实体识别、指代消解、关系抽取等。这些内容在后面的介绍里也会提到。

下面用一个命名实体识别的例子来感受这种转换的关系（原文 vs 实体识别结果）：

- 原文：#王一博的 k50 惊喜# 像素好！运行不错！  
- 实体识别结果：王一博（人名）、k50（产品/型号）……

很多同学做文本，其实最终想要的是“变量”，比如“提没提到某个人/某个组织”“有没有某种态度”“在讨论什么主题”。文本是非结构化的，所以你要做的事情，本质上就是把文本一点点变成你能用的结构化信息。

---

### 1.1.3 歧义性
自然语言的另一个特点是歧义性。我们经常会遇到同一个词在不同语境下含义不同，甚至同一句话也可能有不同理解。比如中文里常见的双关、反讽、阴阳怪气，放到网络语境里会更明显。

所以，很多时候不是模型不够强，而是语言本来就容易“说不清”。这也是为什么做文本分析经常要回到语境里去看，而不是只看词频。

社科语境里还有一种更麻烦的歧义：同一个词在不同群体里含义不同。比如“稳定”“自由”“国家安全”这种词，光看字面你很难判断它到底在表达什么立场。

---

### 1.1.4 容错性
自然语言对人类来说是容错的。你打字打错了、语序不太对、漏了几个字，人也能大概理解。但对计算机来说，很多“差不多”都可能导致完全不同的结果。

这也是为什么文本清洗、统一格式、规范化处理在实际工程里非常重要。你后面会发现，很多效果差异并不是来自“模型多高级”，而是来自“数据前面处理得有多扎实”。

如果你要做可复现的研究，建议把清洗规则写下来：删了什么、替换了什么、为什么这么做。别怕麻烦，后面你自己都会感谢你自己。

---

## 1.2 自然语言处理的方法

自然语言处理的方法大致可以按照：规则、统计、深度学习来分。

### 1.2.1 规则
规则的方法可以理解为“人写规则，机器照着做”。优点是可解释、成本低、上手快，缺点是规则很难穷尽，一旦换领域就容易崩。

不过在很多社科任务里，规则法其实很好用：你要抓一些明确的关键词、实体、或者某类话术，规则法往往就能跑出一个不错的 baseline。

---

### 1.2.2 统计
统计方法的核心思想是：让机器从数据里自己学规律，而不是我们把规律全都写死。常见的分类、聚类、主题模型，很多都是统计方法的延伸。

统计方法一般离不开两个东西：特征（把文本变成数字）和模型（用数学方式学习规律）。

---

### 1.2.3 深度学习
深度学习可以理解为更复杂的函数拟合，它在很多 NLP 任务上带来了明显提升。尤其是后来预训练模型出现之后（比如 BERT 这一类），模型对“语义”的把握更强了。

但深度学习并不是万能的：它更吃数据、更吃算力，也更需要你认真做评估与错误分析。否则很容易出现“看起来很准，但一换语料就不行”的情况。

---

## 1.3 机器学习

机器学习的核心是使用算法解析数据，从中学习，然后对某件事情做出决定或预测。与其直接写死程序，不如让计算机通过样本学习出一个模型。

### 1.3.1 模型
模型就是对问题的一种抽象。最简单的模型可以写成函数形式，比如：

\[
f(x)=wx+b
\]

给定输入 \(x\)，输出一个结果 \(f(x)\)。机器学习的训练过程，就是在数据上不断调整参数（比如 \(w, b\)），让预测更接近真实答案。

---

### 1.3.2 特征
特征就是把对象的特点转换成可计算的数值。对于文本来说，最常见的方式就是把词当作特征。

例如有四句话：

- 我爱你  
- 我恨你  
- 我喜欢你  
- 我讨厌你  

我们可以把“爱/恨/喜欢/讨厌”当作四个特征，那么“我爱你”就可以表示成：

\[
[1,0,0,0]
\]

这就是把文本表示成向量。

（补充一点）后面你会看到更复杂的特征表示，比如 TF-IDF、word2vec、BERT embedding 等。但不管怎么变，本质都是：把文本变成数学对象。

---

### 1.3.3 有监督学习与无监督学习
有监督学习可以理解为：有答案的学习。比如你告诉模型哪些是正样本、哪些是负样本，它就学习如何区分。

无监督学习可以理解为：没有标准答案的学习。它只能发现样本之间的联系，不能学习样本和答案之间的关联。无监督学习一般用于聚类和降维，这些都是不需要标注的。具体的方法也会在后续的章节介绍。

那么，我们到底如何选择监督学习还是无监督学习呢？一般而言有以下的策略：

1. 数据是否有标注？是否有专家支持？  
2. 审查算法，看看是否符合数据量和结构，是否符合维度？  
3. 研究以往的成功案例  

很多时候你可以先用无监督做探索，搞清楚语料大概在说什么；再做监督，把你真正关心的东西做成稳定的变量。

---

## 1.4 准确率评价指标

无论是上文提到的有监督或者无监督学习，还是后面会提到的深度学习，只要我们通过学习得到的模型对数据进行预测，我们都需要一个指标去评价它的准确率。

很多情况下，简单的将正确的数量除以总的数量而得到的正确率并不能让我们满意。例如，假设 100 人中有 1 个人携带了病毒，而医院在检测的时候全部判断为阴性。虽然有 99 人的检测都是正确的，正确率高达 99%，但是这个正确率并不能被我们接受。所以，需要一些更加有效的评价方法。

如果我们用 P 代表正类（阳性），N 代表负类（阴性），那么可以写成一个混淆矩阵：

|            | 预测 P | 预测 N |
|------------|--------|--------|
| 真实 P     | TP     | FN     |
| 真实 N     | FP     | TN     |

表格内容的解释为：

- TP：真阳，答案是 P，预测的结果也是 P；  
- TN：真阴，答案是 N，预测的结果也是 N；  
- FP：假阳，答案是 N，预测的结果是 P；  
- FN：假阴，答案是 P，预测的结果是 N  

有了这些数值，我们就可以引入以下的指标了。

**精确率（precision）**指的是预测结果中正类数占全部预测为正的比率：

\[
Precision=\frac{TP}{TP+FP}
\]

回到之前的病毒检测的问题，精确率直接变成了 0。

**召回率（recall）**指的是正类样本被找出来的比率：

\[
Recall=\frac{TP}{TP+FN}
\]

同样地，在上述的病毒检测的问题中，召回率也是 0。

社科里经常会遇到“稀有现象”（比如极端言论、动员号召、少数派观点）。这时候 accuracy 往往会虚高，precision/recall 才更能反映模型到底有没有用。

---

## 小结
这一章主要想说明两件事：

1. 自然语言本身就很复杂：词汇量大、非结构化、歧义强、而且人类容错但机器不太容错。  
2. NLP 的方法大致可以按规则、统计、深度学习来理解；而机器学习的核心就是“特征 + 模型 + 评估”。

后面的章节我们会从分词开始，把这些东西一点点落到实战里。


